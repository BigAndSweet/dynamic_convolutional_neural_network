{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pprint import pprint\n",
    "import math\n",
    "import nltk\n",
    "SEED = 1234\n",
    "# import spacy\n",
    "import torch.optim as optim\n",
    "import pyprind\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:14\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "glove2embedding = {}\n",
    "\n",
    "bar_count = len(open(f'/home/neo/glove.6B.50d.txt', 'rb').readlines())\n",
    "\n",
    "with open(f'/home/neo/glove.6B.50d.txt', 'rb') as f:\n",
    "    bar = pyprind.ProgBar(bar_count, bar_char='█')\n",
    "    for idx, l in enumerate(f):\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "#         words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        \n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        glove2embedding[word] = vect\n",
    "        idx += 1\n",
    "        bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size is 400000.\n"
     ]
    }
   ],
   "source": [
    "print(\"Total vocabulary size is {}.\".format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2embedding[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data is (79654, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>constructs a hilarious ode to middle america a...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this quality band may pick up new admirers</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a sloppy slapstick</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finest</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much fun</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split\n",
       "0  constructs a hilarious ode to middle america a...      1  train\n",
       "1         this quality band may pick up new admirers      1  train\n",
       "2                                 a sloppy slapstick      0  train\n",
       "3                                             finest      1  train\n",
       "4                                           much fun      1  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/neo/sentiment_datasets/SST2.csv\")\n",
    "df = df.sample(n = df.shape[0]).reset_index(drop=True)\n",
    "print(\"shape of the data is {}\".format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    0\n",
       "label       0\n",
       "split       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"sentence\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    76961\n",
       "test      1821\n",
       "dev        872\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    43612\n",
       "0    36042\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79654/79654 [00:09<00:00, 8549.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>constructs a hilarious ode to middle america a...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this quality band may pick up new admirers</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a sloppy slapstick</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finest</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much fun</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split  \\\n",
       "0  constructs a hilarious ode to middle america a...      1  train   \n",
       "1         this quality band may pick up new admirers      1  train   \n",
       "2                                 a sloppy slapstick      0  train   \n",
       "3                                             finest      1  train   \n",
       "4                                           much fun      1  train   \n",
       "\n",
       "   text_length  \n",
       "0           27  \n",
       "1            8  \n",
       "2            3  \n",
       "3            1  \n",
       "4            2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_length\"] = df.sentence.progress_apply(lambda x: len(nltk.word_tokenize(x.strip())))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff981a66940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.text_length.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2idx(list_of_text):\n",
    "    words = {\"_pad_\": 0}\n",
    "    idx = 1 \n",
    "    for i in list_of_text:\n",
    "        for w in nltk.word_tokenize(i):\n",
    "            if w.lower() not in words.keys():\n",
    "                words[w.lower()] = idx\n",
    "                idx += 1\n",
    "    print(f'Number of unique tokens in the data are {len(words)}')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the data are 17290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'with'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = create_word2idx(df.sentence)\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "idx2word[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"word2indexed\"] = df.review.apply(lambda x: [words[w.lower()] for w in nltk.word_tokenize(x.strip())])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(EMBEDDING_DIM, word2idx, embedding_function):\n",
    "    matrix_len = len(list(word2idx.keys()))\n",
    "    weights_matrix = np.zeros((matrix_len, EMBEDDING_DIM), dtype=float)\n",
    "    words_found = 0\n",
    "    bar = pyprind.ProgBar(len(word2idx), bar_char='█')\n",
    "    for i, word in enumerate(list(word2idx.keys())):\n",
    "        try: \n",
    "            weights_matrix[i] = glove2embedding[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        bar.update()\n",
    "    print(f\"Number of words from text found in embedding function are {words_found}\")\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from text found in embedding function are 16841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_WEIGHTS_MATRIX = create_embedding_matrix(EMBEDDING_DIM, word2idx, glove2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df=None, maxlen=10, word2idx=None):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "        self.df['sentence'] = self.df.sentence.progress_apply(lambda x: x.strip())\n",
    "        print('Indexing...')\n",
    "        self.df['sentimentidx'] = self. df.sentence.progress_apply(lambda x: [word2idx[w.lower()] for w in nltk.word_tokenize(x.strip())])\n",
    "        print('Calculating lengths')\n",
    "        self.df['lengths'] = self.df.sentimentidx.progress_apply(lambda x: self.maxlen if len(x) > self.maxlen else len(x))\n",
    "        print('Padding')\n",
    "        self.df['sentimentpadded'] = self.df.sentimentidx.progress_apply(self.pad_data)\n",
    "#         print(self.df.head())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df.sentimentpadded[idx]\n",
    "        y = self.df.label[idx]\n",
    "        return X, y\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76961, 4)\n",
      "(1821, 4)\n",
      "(872, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = df.loc[df.split == \"train\", :].reset_index(drop=True)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = df.loc[df.split == \"test\", :].reset_index(drop=True)\n",
    "print(df_test.shape)\n",
    "\n",
    "df_dev = df.loc[df.split == \"dev\", :].reset_index(drop=True)\n",
    "print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76961/76961 [00:00<00:00, 679402.74it/s]\n",
      "  1%|          | 705/76961 [00:00<00:10, 7049.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76961/76961 [00:09<00:00, 8011.61it/s]\n",
      "100%|██████████| 76961/76961 [00:00<00:00, 660334.32it/s]\n",
      "  0%|          | 0/76961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating lengths\n",
      "Padding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76961/76961 [00:00<00:00, 206361.07it/s]\n",
      "100%|██████████| 1821/1821 [00:00<00:00, 620054.20it/s]\n",
      " 57%|█████▋    | 1045/1821 [00:00<00:00, 4967.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [00:00<00:00, 5348.40it/s]\n",
      "100%|██████████| 1821/1821 [00:00<00:00, 407415.99it/s]\n",
      "100%|██████████| 1821/1821 [00:00<00:00, 128505.07it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 322866.62it/s]\n",
      " 61%|██████    | 530/872 [00:00<00:00, 5298.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating lengths\n",
      "Padding\n",
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [00:00<00:00, 5263.91it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 302517.21it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 91842.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating lengths\n",
      "Padding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_train = VectorizeData(df_train, maxlen=30, word2idx=word2idx)\n",
    "ds_test = VectorizeData(df_test, maxlen=30, word2idx=word2idx)\n",
    "ds_dev = VectorizeData(df_dev, maxlen=30, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2406\n",
      "57\n",
      "28\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "dl_train = DataLoader(dataset=ds_train, batch_size=32, shuffle=True)\n",
    "print(len(dl_train))\n",
    "\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=True)\n",
    "print(len(dl_test))\n",
    "# 526204\n",
    "\n",
    "dl_dev = DataLoader(dataset=ds_dev, batch_size=32, shuffle=True)\n",
    "print(len(dl_dev))\n",
    "\n",
    "it = iter(dl_train)\n",
    "xs,ys =  next(it)\n",
    "print(type(xs))\n",
    "# print(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(num_embeddings=len(word2idx), embedding_dim=EMBEDDING_DIM)\n",
    "# embedded = embedding(xs)\n",
    "# print(embedded.shape)\n",
    "\n",
    "# embedded = embedded.unsqueeze(1)\n",
    "# embedded.shape\n",
    "\n",
    "# co = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 1), stride=(1,1), padding=(2, 0))\n",
    "# c = co(embedded)\n",
    "# c.shape\n",
    "\n",
    "# pool_1 = torch.topk(c, 5, dim=2, largest=True)[0]\n",
    "# pool_1.shape\n",
    "\n",
    "# co_2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(2, 1), stride=(1,1), padding=(1, 0))\n",
    "# c_2 = co_2(pool_1)\n",
    "# c_2.shape\n",
    "\n",
    "# fold = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "# f = fold(c_2)\n",
    "# f.shape\n",
    "\n",
    "# pool_2 = torch.topk(f, 3, dim=2, largest=True)[0]\n",
    "# pool_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution 2d \n",
    "- kmax pooling\n",
    "- non_linear function tanh to the pooled matrix \n",
    "- second convolution\n",
    "- folding\n",
    "- kmax pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_number=1,\n",
    "        sent_length=7,\n",
    "        conv_kernel_size=(3, 1),\n",
    "        conv_input_channels=1,\n",
    "        conv_output_channels=2,\n",
    "        conv_stride=(1, 1),\n",
    "        k_max_number=5,\n",
    "        folding_kernel_size=(1, 2),\n",
    "        folding_stride=(1,1)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cell_number=cell_number \n",
    "        self.sent_length=sent_length\n",
    "        self.conv_kernel_size=conv_kernel_size\n",
    "        self.conv_input_channels=conv_input_channels\n",
    "        self.conv_output_channels=conv_output_channels\n",
    "        self.conv_stride=conv_stride\n",
    "        self.k_max_number=k_max_number\n",
    "        self.folding_kernel_size=folding_kernel_size\n",
    "        self.folding_stride=folding_stride\n",
    "        \n",
    "        # calculating padding size\n",
    "        self.pad_0_direction = math.ceil(self.conv_kernel_size[0]  - 1)\n",
    "        self.pad_1_direction = math.ceil(self.conv_kernel_size[1] - 1)\n",
    "        \n",
    "        # 2d convolution\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels=self.conv_input_channels,\n",
    "            out_channels=self.conv_output_channels,\n",
    "            kernel_size=self.conv_kernel_size,\n",
    "            stride=self.conv_stride,\n",
    "            padding=(self.pad_0_direction, self.pad_1_direction)\n",
    "        )\n",
    "        \n",
    "        # if cell is last then initialising folding\n",
    "        if cell_number == -1:\n",
    "            self.fold = nn.AvgPool2d(kernel_size=self.folding_kernel_size, stride=self.folding_stride)\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # [batch_size, input_channels, sent_length_in, embedding_dim]\n",
    "        conved = self.conv_layer(inp)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length_out, embedding_dim]\n",
    "        if self.cell_number == -1:\n",
    "            conved = self.fold(conved)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length, embedding_dim/2]\n",
    "        k_maxed = torch.tanh(torch.topk(conved, self.k_max_number, dim=2, largest=True)[0])\n",
    "        \n",
    "        # [batch_size, out_channels, k_maxed_number, embedding_dim/2]\n",
    "        return k_maxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell = DCNNCell()\n",
    "# out = cell(embedded)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_last = DCNNCell(\n",
    "#     cell_number=-1,\n",
    "#     sent_length=5,\n",
    "#     conv_kernel_size=(2, 1),\n",
    "#     conv_input_channels=2,\n",
    "#     conv_output_channels=6,\n",
    "#     conv_stride=(1, 1),\n",
    "#     k_max_number=3,\n",
    "#     folding_kernel_size=(1, 2),\n",
    "#     folding_stride=(1,2)\n",
    "# )\n",
    "# out_2 = cell_last(out)\n",
    "# out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "class DCNN_SST(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameter_dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.parameter_dict = parameter_dict\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            embedding_dim=self.parameter_dict[\"embedding_dim\"],\n",
    "            num_embeddings=self.parameter_dict[\"vocab_length\"]\n",
    "        )\n",
    "        \n",
    "        self.dcnn_first_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_one_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_one_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_stride\"],\n",
    "        )\n",
    "        \n",
    "        self.dcnn_last_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_two_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_stride\"],\n",
    "        )\n",
    "        \n",
    "        self.fc_layer_input = self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"] *\\\n",
    "            self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"] *\\\n",
    "            math.floor(self.parameter_dict[\"embedding_dim\"]/4)\n",
    "            \n",
    "        self.dropout = nn.Dropout(self.parameter_dict[\"dropout_rate\"])\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc = nn.Linear(self.fc_layer_input, self.parameter_dict[\"output_dim\"])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # [batch_size, sent_length]\n",
    "        embedded = self.embedding(inp)\n",
    "        \n",
    "        # [batch_size, sent_length, embedding_dim]\n",
    "        # adding single channel dimension\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "#         print(embedded.shape)\n",
    "        \n",
    "        # [batch_size, 1(initial_input_channel), sent_length, embedding_dim]\n",
    "        out = self.dcnn_first_cell(embedded)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, first_cell_output_channels, first_cell_k_maxed_number, embedding_dim]\n",
    "        out = self.dcnn_last_cell(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, last_cell_output_channels, last_cell_k_maxed_number, embedding_dim/2]\n",
    "        flat = self.dropout(self.flatten(out))\n",
    "#         print(flat.shape)\n",
    "\n",
    "        #[batch_size, last_cell_output_channels * last_cell_k_maxed_number * embedding_dim/2]\n",
    "        fc = self.fc(flat)\n",
    "#         print(fc.shape)\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_one_parameter_dict': {'conv_input_channels': 1,\n",
      "                             'conv_kernel_size': (7, 1),\n",
      "                             'conv_output_channels': 6,\n",
      "                             'conv_stride': (1, 1),\n",
      "                             'folding_kernel_size': (1, 2),\n",
      "                             'folding_stride': (1, 2),\n",
      "                             'k_max_number': 10,\n",
      "                             'sent_length': 20},\n",
      " 'cell_two_parameter_dict': {'conv_input_channels': 6,\n",
      "                             'conv_kernel_size': (5, 1),\n",
      "                             'conv_output_channels': 14,\n",
      "                             'conv_stride': (1, 1),\n",
      "                             'folding_kernel_size': (1, 2),\n",
      "                             'folding_stride': (1, 2),\n",
      "                             'k_max_number': 4,\n",
      "                             'sent_length': 10},\n",
      " 'dropout_rate': 0.2,\n",
      " 'embedding_dim': 50,\n",
      " 'output_dim': 3,\n",
      " 'vocab_length': 17290}\n"
     ]
    }
   ],
   "source": [
    "SST_DATASET_PARAMETERS = {\n",
    "    \"cell_one_parameter_dict\" : {\n",
    "        \"sent_length\": 20,\n",
    "        \"conv_kernel_size\": (7, 1),\n",
    "        \"conv_input_channels\": 1,\n",
    "        \"conv_output_channels\": 6,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 10,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"cell_two_parameter_dict\" : {\n",
    "        \"sent_length\": None,\n",
    "        \"conv_kernel_size\": (5, 1),\n",
    "        \"conv_input_channels\": 6,\n",
    "        \"conv_output_channels\": 14,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 4,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"embedding_dim\": 50,\n",
    "    \"vocab_length\": len(word2idx),\n",
    "    \"output_dim\": 3\n",
    "}\n",
    "SST_DATASET_PARAMETERS[\"cell_two_parameter_dict\"][\"sent_length\"] = SST_DATASET_PARAMETERS[\"cell_one_parameter_dict\"][\"k_max_number\"]\n",
    "pprint(SST_DATASET_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCNN_SST(\n",
    "    parameter_dict=SST_DATASET_PARAMETERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(xs)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2132, -0.6645, -0.4087,  ...,  0.4081,  1.0903,  0.0903],\n",
       "        [ 0.7490, -0.0691, -0.2080,  ...,  0.5058,  0.1231, -0.2510],\n",
       "        [ 0.2171,  0.4651, -0.4676,  ..., -0.0438,  0.4101,  0.1796],\n",
       "        ...,\n",
       "        [ 0.1909, -0.3019, -0.1181,  ...,  0.4778, -1.1525, -0.2887],\n",
       "        [ 0.8339,  0.4576,  0.5947,  ..., -0.1194,  0.0247, -0.4135],\n",
       "        [ 0.4891, -0.9495,  0.6580,  ..., -0.4978, -0.2764, -0.9567]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(torch.from_numpy(EMBEDDING_WEIGHTS_MATRIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model are : 867001\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Number of trainable parameters in the model are : {}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yntY13-w6UMa"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
    "    correct = (ind == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-sJsioa6UMh"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#     bar = pyprind.ProgBar(100, bar_char='█')\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "#         x, y = batch\n",
    "\n",
    "        inputs, labels = batch\n",
    "        if device == None:\n",
    "            x, y = Variable(inputs), Variable(labels.long())\n",
    "        else:\n",
    "            x, y = Variable(inputs.cuda()), Variable(labels.long().cuda())\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = calculate_accuracy(predictions, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        bar.update()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJbOdv4-6UMt"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#         bar = pyprind.ProgBar(100, bar_char='█')\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            inputs, labels = batch\n",
    "            if device == None:\n",
    "                x, y = Variable(inputs), Variable(labels.long())\n",
    "            else:\n",
    "                x, y = Variable(inputs.cuda()), Variable(labels.long().cuda())\n",
    "            \n",
    "\n",
    "            predictions = model(x).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, y)\n",
    "            acc = calculate_accuracy(predictions, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            bar.update()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation loss did not improve\")\n",
    "    return\n",
    "\n",
    "MODEL_PATH = \"/home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3143
    },
    "colab_type": "code",
    "id": "qFKq12GF6UM4",
    "outputId": "a0482869-2aeb-47e7-b550-dcd8dc3a0694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:58\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 01 | Train Loss: 0.427 | Train Acc: 78.64% | Val. Loss: 0.423 | Val. Acc: 81.70% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 02 | Train Loss: 0.221 | Train Acc: 91.48% | Val. Loss: 0.484 | Val. Acc: 80.13% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:57\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 03 | Train Loss: 0.159 | Train Acc: 94.17% | Val. Loss: 0.465 | Val. Acc: 81.36% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:57\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 04 | Train Loss: 0.126 | Train Acc: 95.47% | Val. Loss: 0.520 | Val. Acc: 81.47% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 05 | Train Loss: 0.105 | Train Acc: 96.26% | Val. Loss: 0.587 | Val. Acc: 80.80% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 06 | Train Loss: 0.088 | Train Acc: 96.88% | Val. Loss: 0.605 | Val. Acc: 81.25% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:57\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 07 | Train Loss: 0.077 | Train Acc: 97.34% | Val. Loss: 0.664 | Val. Acc: 79.35% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 08 | Train Loss: 0.069 | Train Acc: 97.60% | Val. Loss: 0.683 | Val. Acc: 80.58% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:58\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 09 | Train Loss: 0.061 | Train Acc: 97.90% | Val. Loss: 0.703 | Val. Acc: 80.80% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:58\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 10 | Train Loss: 0.057 | Train Acc: 98.07% | Val. Loss: 0.731 | Val. Acc: 80.47% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "base_dev_acc = 0.0\n",
    "for epoch in range(N_EPOCHS):\n",
    "    is_best = False\n",
    "    \n",
    "    train_loss, train_acc = train(model, dl_train, optimizer, criterion, device=device)\n",
    "    valid_loss, valid_acc = evaluate(model, dl_dev, criterion, device=device)\n",
    "\n",
    "    print()\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    \n",
    "    if base_dev_acc < valid_acc:\n",
    "        is_best = True\n",
    "        base_dev_acc = valid_acc\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'best_loss': valid_loss,\n",
    "        'best_dev_accuracy_sentiment': valid_acc\n",
    "    }, is_best, MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_weights = MODEL_PATH\n",
    "checkpoint = torch.load(resume_weights)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy_sentiment = checkpoint['best_dev_accuracy_sentiment']\n",
    "best_accuracy_topic = checkpoint['best_dev_accuracy_topic']\n",
    "shared_model.load_state_dict(checkpoint['state_dict_shared_model'])\n",
    "sentiment_specific_network.load_state_dict(checkpoint['state_dict_sentiment_specific_network'])\n",
    "topic_specific_network.load_state_dict(checkpoint['state_dict_topic_specific_network'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_on_given_set(model_list, df, SENT_LENGTH, WORD2IDX, device=\"cpu\"):\n",
    "#     ds_test = VectorizeData(df_test, maxlen=SENT_LENGTH, word2idx=WORD2IDX)\n",
    "#     dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model_list = [model.eval() for model in model_list]\n",
    "    shared_model = model_list[0]\n",
    "    sentiment_specific_model = model_list[1]\n",
    "    topic_specific_model = model_list[2]\n",
    "    \n",
    "    y_test_sentiment = []\n",
    "    y_test_topic = []\n",
    "    all_prediction_sentiment = []\n",
    "    all_prediction_topic = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(dl_test), bar_char='█')\n",
    "#         bar = pyprind.ProgBar(100, bar_char='█')\n",
    "        \n",
    "        for i, batch in enumerate(dl_test):\n",
    "            inputs, labels_sentiment, labels_topic = batch\n",
    "            if device == \"cpu\":\n",
    "                x, y_sentiment, y_topic = Variable(inputs), Variable(labels_sentiment.long()), Variable(labels_topic.long())\n",
    "            else:\n",
    "                x, y_sentiment, y_topic = Variable(inputs.cuda()), Variable(labels_sentiment.long().cuda()), Variable(labels_topic.long().cuda())\n",
    "            \n",
    "            # shared model output\n",
    "            shared_model_output = shared_model(x)\n",
    "\n",
    "            sentiment_specific_output = sentiment_specific_model(shared_model_output).squeeze(1)\n",
    "            topic_specific_output = topic_specific_model(shared_model_output).squeeze(1)\n",
    "\n",
    "            predictions_sentiment = return_predictions(sentiment_specific_output)\n",
    "            predictions_topic = return_predictions(topic_specific_output)\n",
    "\n",
    "            all_prediction_sentiment.extend(predictions_sentiment.tolist())\n",
    "            all_prediction_topic.extend(predictions_topic.tolist())\n",
    "            \n",
    "            y_test_sentiment.extend(y_sentiment.tolist())\n",
    "            y_test_topic.extend(y_topic.tolist())\n",
    "            \n",
    "            bar.update()\n",
    "\n",
    "    print(\"Sentiment REsutls ::::::::::::::::::::::::;\")\n",
    "    print( 'Accuracy:', accuracy_score(y_test_sentiment, all_prediction_sentiment))\n",
    "\n",
    "    print(\"macro f1 score : {}\".format(f1_score(y_test_sentiment, all_prediction_sentiment, average='macro')))\n",
    "\n",
    "    print(\"micro f1 score : {}\".format(f1_score(y_test_sentiment, all_prediction_sentiment, average='micro')))      \n",
    "\n",
    "    print( '\\n clasification report:\\n', classification_report(y_test_sentiment, all_prediction_sentiment))\n",
    "\n",
    "    print( '\\n confusion matrix:\\n',confusion_matrix(y_test_sentiment, all_prediction_sentiment))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"TOPIC REsutls ::::::::::::::::::::::::;\")\n",
    "    print( 'Accuracy:', accuracy_score(y_test_topic, all_prediction_topic))\n",
    "\n",
    "    print(\"macro f1 score : {}\".format(f1_score(y_test_topic, all_prediction_topic, average='macro')))\n",
    "\n",
    "    print(\"micro f1 score : {}\".format(f1_score(y_test_topic, all_prediction_topic, average='micro')))      \n",
    "\n",
    "    print( '\\n clasification report:\\n', classification_report(y_test_topic, all_prediction_topic))\n",
    "\n",
    "    print( '\\n confusion matrix:\\n',confusion_matrix(y_test_topic, all_prediction_topic))\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
