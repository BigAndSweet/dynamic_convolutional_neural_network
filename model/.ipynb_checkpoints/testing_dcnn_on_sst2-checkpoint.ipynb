{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pprint import pprint\n",
    "import math\n",
    "import nltk\n",
    "SEED = 1234\n",
    "# import spacy\n",
    "import torch.optim as optim\n",
    "import pyprind\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(embedding_file_path):\n",
    "    word2idx = {}\n",
    "    glove2embedding = {}\n",
    "    bar_count = len(open(embedding_file_path, 'rb').readlines())\n",
    "    with open(embedding_file_path, 'rb') as f:\n",
    "        bar = pyprind.ProgBar(bar_count, bar_char='█')\n",
    "        for idx, l in enumerate(f):\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "#             words.append(word)\n",
    "            word2idx[word] = idx\n",
    "\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            glove2embedding[word] = vect\n",
    "            idx += 1\n",
    "            bar.update()\n",
    "    print(\"Total vocabulary size of Embedding model is {}.\".format(len(word2idx)))\n",
    "    return glove2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size of Embedding model is 400000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:25\n"
     ]
    }
   ],
   "source": [
    "embedding_file_path = f'/home/neo/glove.6B.100d.txt'\n",
    "glove2embedding = load_glove_model(embedding_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2embedding[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data is (79654, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in alcatraz ' ... a cinematic corpse</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they were insulted and the audience was put th...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at a bad rock concert</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>made eddie murphy a movie star and the man</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the movie 's downfall is to substitute plot fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split\n",
       "0               in alcatraz ' ... a cinematic corpse      0  train\n",
       "1  they were insulted and the audience was put th...      0  train\n",
       "2                              at a bad rock concert      0  train\n",
       "3         made eddie murphy a movie star and the man      1  train\n",
       "4  the movie 's downfall is to substitute plot fo...      0  train"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/neo/sentiment_datasets/SST2.csv\")\n",
    "df = df.sample(n = df.shape[0]).reset_index(drop=True)\n",
    "print(\"shape of the data is {}\".format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    0\n",
       "label       0\n",
       "split       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    43612\n",
       "0    36042\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    76961\n",
       "test      1821\n",
       "dev        872\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    43612\n",
       "0    36042\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79654/79654 [00:09<00:00, 8515.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an affable but undernourished romantic comedy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can report that it is instead a cheap clich</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a mildly funny , sometimes tedious , ultimatel...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>over every cheap trick in the book trying to m...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>animated by an energy that puts the dutiful ef...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  split  \\\n",
       "0  an affable but undernourished romantic comedy ...      0  train   \n",
       "1        can report that it is instead a cheap clich      0  train   \n",
       "2  a mildly funny , sometimes tedious , ultimatel...      0  train   \n",
       "3  over every cheap trick in the book trying to m...      0  train   \n",
       "4  animated by an energy that puts the dutiful ef...      1  train   \n",
       "\n",
       "   text_length  \n",
       "0           18  \n",
       "1            9  \n",
       "2            9  \n",
       "3           12  \n",
       "4           14  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_length\"] = df.sentence.progress_apply(lambda x: len(nltk.word_tokenize(x.strip())))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6461144e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFupJREFUeJzt3X+s3XWd5/Hnawoo0XEBkZumZbdstn+Idq16A03cP+6ggYKTKZNoAmGH6pB01kBWk86u1X8YQRJMVtmQKEln6VI2jkhUlkbrMg1y4poovxSBioYOdqVTlsYtKFezmLrv/eN86p7t99wfvfe2995zn4/k5J7v+3y+3/N5e4993e+P8yVVhSRJg/5osScgSVp6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOs5Y7AnM1fnnn1/r1q2bdsxvfvMb3vSmN52eCS2ildCnPY6OldDnUu7xySef/GVVvW2mccs2HNatW8cTTzwx7Zher8fExMTpmdAiWgl92uPoWAl9LuUek/yP2YzzsJIkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKlj2X5Dej7W7fjWorzvwds/uCjvK0knyz0HSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI4ZwyHJG5M8luTHSfYn+Uyr35Pk50meao+NrZ4kdyY5kOTpJO8Z2NbWJM+3x9aB+nuTPNPWuTNJTkWzkqTZmc33HF4HLquqySRnAt9L8u322r+rqq+dMP5KYH17XArcBVya5DzgZmAcKODJJHuq6pU2ZhvwA2AvsBn4NpKkRTHjnkP1TbbFM9ujplllC3BvW+8HwDlJVgNXAPuq6mgLhH3A5vbaW6rq+1VVwL3A1fPoSZI0T7M655BkVZKngCP0/4F/tL10Wzt0dEeSN7TaGuDFgdUPtdp09UND6pKkRTKr22dU1e+BjUnOAR5I8k7gU8D/BM4CdgKfBG4Bhp0vqDnUO5Jso3/4ibGxMXq93rTznpycHDpm+4Zj0653qsw037maqs9RYo+jYyX0OQo9ntS9larq1SQ9YHNV/YdWfj3Jfwb+ui0fAi4cWG0tcLjVJ06o91p97ZDxw95/J/0gYnx8vCYmJoYN+4Ner8ewMR9ZrHsrXTdxSrY7VZ+jxB5Hx0rocxR6nM3VSm9rewwkORv4APDTdq6AdmXR1cCzbZU9wPXtqqVNwK+q6iXgIeDyJOcmORe4HHiovfZakk1tW9cDDy5sm5KkkzGbPYfVwO4kq+iHyf1V9c0k30nyNvqHhZ4C/k0bvxe4CjgA/Bb4KEBVHU1yK/B4G3dLVR1tzz8G3AOcTf8qJa9UkqRFNGM4VNXTwLuH1C+bYnwBN07x2i5g15D6E8A7Z5qLJOn08BvSkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjpmDIckb0zyWJIfJ9mf5DOtflGSR5M8n+SrSc5q9Te05QPt9XUD2/pUq/8syRUD9c2tdiDJjoVvU5J0Mmaz5/A6cFlVvQvYCGxOsgn4HHBHVa0HXgFuaONvAF6pqn8B3NHGkeRi4BrgHcBm4EtJViVZBXwRuBK4GLi2jZUkLZIZw6H6Jtvime1RwGXA11p9N3B1e76lLdNef3+StPp9VfV6Vf0cOABc0h4HquqFqvodcF8bK0laJLM659D+wn8KOALsA/4BeLWqjrUhh4A17fka4EWA9vqvgLcO1k9YZ6q6JGmRnDGbQVX1e2BjknOAB4C3DxvWfmaK16aqDwuoGlIjyTZgG8DY2Bi9Xm/aeU9OTg4ds33Dse7g02Cm+c7VVH2OEnscHSuhz1HocVbhcFxVvZqkB2wCzklyRts7WAscbsMOARcCh5KcAfwT4OhA/bjBdaaqn/j+O4GdAOPj4zUxMTHtfHu9HsPGfGTHt6Zd71Q5eN3EKdnuVH2OEnscHSuhz1HocTZXK72t7TGQ5GzgA8BzwCPAh9qwrcCD7fmetkx7/TtVVa1+Tbua6SJgPfAY8Diwvl39dBb9k9Z7FqI5SdLczGbPYTWwu11V9EfA/VX1zSQ/Ae5L8lngR8DdbfzdwH9JcoD+HsM1AFW1P8n9wE+AY8CN7XAVSW4CHgJWAbuqav+CdShJOmkzhkNVPQ28e0j9BfpXGp1Y/9/Ah6fY1m3AbUPqe4G9s5ivJOk08BvSkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjpmDIckFyZ5JMlzSfYn+Xir/02Sf0zyVHtcNbDOp5IcSPKzJFcM1De32oEkOwbqFyV5NMnzSb6a5KyFblSSNHuz2XM4BmyvqrcDm4Abk1zcXrujqja2x16A9to1wDuAzcCXkqxKsgr4InAlcDFw7cB2Pte2tR54BbhhgfqTJM3BjOFQVS9V1Q/b89eA54A106yyBbivql6vqp8DB4BL2uNAVb1QVb8D7gO2JAlwGfC1tv5u4Oq5NiRJmr+TOueQZB3wbuDRVropydNJdiU5t9XWAC8OrHao1aaqvxV4taqOnVCXJC2SM2Y7MMmbga8Dn6iqXye5C7gVqPbz88BfAhmyejE8iGqa8cPmsA3YBjA2Nkav15t2zpOTk0PHbN9wrDv4NJhpvnM1VZ+jxB5Hx0rocxR6nFU4JDmTfjB8uaq+AVBVLw+8/rfAN9viIeDCgdXXAofb82H1XwLnJDmj7T0Mjv//VNVOYCfA+Ph4TUxMTDvvXq/HsDEf2fGtadc7VQ5eN3FKtjtVn6PEHkfHSuhzFHqczdVKAe4GnquqLwzUVw8M+3Pg2fZ8D3BNkjckuQhYDzwGPA6sb1cmnUX/pPWeqirgEeBDbf2twIPza0uSNB+z2XN4H/AXwDNJnmq1T9O/2mgj/UNAB4G/Aqiq/UnuB35C/0qnG6vq9wBJbgIeAlYBu6pqf9veJ4H7knwW+BH9MJIkLZIZw6Gqvsfw8wJ7p1nnNuC2IfW9w9arqhfoX80kSVoC/Ia0JKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI4ZwyHJhUkeSfJckv1JPt7q5yXZl+T59vPcVk+SO5McSPJ0kvcMbGtrG/98kq0D9fcmeaatc2eSYf/NaknSaTKbPYdjwPaqejuwCbgxycXADuDhqloPPNyWAa4E1rfHNuAu6IcJcDNwKXAJcPPxQGljtg2st3n+rUmS5mrGcKiql6rqh+35a8BzwBpgC7C7DdsNXN2ebwHurb4fAOckWQ1cAeyrqqNV9QqwD9jcXntLVX2/qgq4d2BbkqRFcFLnHJKsA94NPAqMVdVL0A8Q4II2bA3w4sBqh1ptuvqhIXVJ0iI5Y7YDk7wZ+Drwiar69TSnBYa9UHOoD5vDNvqHnxgbG6PX600758nJyaFjtm84Nu16p8pM852rqfocJfY4OlZCn6PQ46zCIcmZ9IPhy1X1jVZ+OcnqqnqpHRo60uqHgAsHVl8LHG71iRPqvVZfO2R8R1XtBHYCjI+P18TExLBhf9Dr9Rg25iM7vjXteqfKwesmTsl2p+pzlNjj6FgJfY5Cj7O5WinA3cBzVfWFgZf2AMevONoKPDhQv75dtbQJ+FU77PQQcHmSc9uJ6MuBh9prryXZ1N7r+oFtSZIWwWz2HN4H/AXwTJKnWu3TwO3A/UluAH4BfLi9the4CjgA/Bb4KEBVHU1yK/B4G3dLVR1tzz8G3AOcDXy7PSRJi2TGcKiq7zH8vADA+4eML+DGKba1C9g1pP4E8M6Z5iJJOj38hrQkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdcz6lt2av3Wn6G6w2zccm/FOswdv/+ApeW9Jo8k9B0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6ZgyHJLuSHEny7EDtb5L8Y5Kn2uOqgdc+leRAkp8luWKgvrnVDiTZMVC/KMmjSZ5P8tUkZy1kg5KkkzebPYd7gM1D6ndU1cb22AuQ5GLgGuAdbZ0vJVmVZBXwReBK4GLg2jYW4HNtW+uBV4Ab5tOQJGn+ZgyHqvoucHSW29sC3FdVr1fVz4EDwCXtcaCqXqiq3wH3AVuSBLgM+Fpbfzdw9Un2IElaYPM553BTkqfbYadzW20N8OLAmEOtNlX9rcCrVXXshLokaRHN9cZ7dwG3AtV+fh74SyBDxhbDQ6imGT9Ukm3ANoCxsTF6vd60k5ycnBw6ZvuGY93By9jY2TP3NNP/VkvdVL/LUbISeoSV0eco9DincKiql48/T/K3wDfb4iHgwoGha4HD7fmw+i+Bc5Kc0fYeBscPe9+dwE6A8fHxmpiYmHaevV6PYWNmuoPpcrN9wzE+/8z0v8qD102cnsmcIlP9LkfJSugRVkafo9DjnA4rJVk9sPjnwPErmfYA1yR5Q5KLgPXAY8DjwPp2ZdJZ9E9a76mqAh4BPtTW3wo8OJc5SZIWzox7Dkm+AkwA5yc5BNwMTCTZSP8Q0EHgrwCqan+S+4GfAMeAG6vq9207NwEPAauAXVW1v73FJ4H7knwW+BFw94J1J0makxnDoaquHVKe8h/wqroNuG1IfS+wd0j9BfpXM0mSlgi/IS1J6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpY8ZwSLIryZEkzw7UzkuyL8nz7ee5rZ4kdyY5kOTpJO8ZWGdrG/98kq0D9fcmeaatc2eSLHSTkqSTM5s9h3uAzSfUdgAPV9V64OG2DHAlsL49tgF3QT9MgJuBS4FLgJuPB0obs21gvRPfS5J0mp0x04Cq+m6SdSeUtwAT7fluoAd8stXvraoCfpDknCSr29h9VXUUIMk+YHOSHvCWqvp+q98LXA18ez5NqWvdjm8tyvsevP2Di/K+kuZnruccxqrqJYD284JWXwO8ODDuUKtNVz80pC5JWkQz7jmcpGHnC2oO9eEbT7bRPwTF2NgYvV5v2slMTk4OHbN9w7Fp11tuxs5euj3N9Duaral+l6NkJfQIK6PPUehxruHwcpLVVfVSO2x0pNUPARcOjFsLHG71iRPqvVZfO2T8UFW1E9gJMD4+XhMTE1MNBfr/MA0b85FFOsRyqmzfcIzPP7PQOb8wDl43sSDbmep3OUpWQo+wMvochR7nelhpD3D8iqOtwIMD9evbVUubgF+1w04PAZcnObediL4ceKi99lqSTe0qpesHtiVJWiQz/rmZ5Cv0/+o/P8kh+lcd3Q7cn+QG4BfAh9vwvcBVwAHgt8BHAarqaJJbgcfbuFuOn5wGPkb/iqiz6Z+I9mS0JC2y2VytdO0UL71/yNgCbpxiO7uAXUPqTwDvnGkekqTTx29IS5I6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktSxNG/Io5GxULcK377h2EnfE8vbhUtz556DJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHXMKxySHEzyTJKnkjzRaucl2Zfk+fbz3FZPkjuTHEjydJL3DGxnaxv/fJKt82tJkjRfC7Hn8CdVtbGqxtvyDuDhqloPPNyWAa4E1rfHNuAu6IcJcDNwKXAJcPPxQJEkLY5TcVhpC7C7Pd8NXD1Qv7f6fgCck2Q1cAWwr6qOVtUrwD5g8ymYlyRpluYbDgX8fZInk2xrtbGqegmg/byg1dcALw6se6jVpqpLkhbJfG/Z/b6qOpzkAmBfkp9OMzZDajVNvbuBfgBtAxgbG6PX6007ucnJyaFjtm84Nu16y83Y2aPX04nm0uNMn4+lZqrP66hZCX2OQo/zCoeqOtx+HknyAP1zBi8nWV1VL7XDRkfa8EPAhQOrrwUOt/rECfXeFO+3E9gJMD4+XhMTE8OG/UGv12PYmJP97wIsdds3HOPzz4z2f5pjLj0evG7i1EzmFJnq8zpqVkKfo9DjnA8rJXlTkj8+/hy4HHgW2AMcv+JoK/Bge74HuL5dtbQJ+FU77PQQcHmSc9uJ6MtbTZK0SObz5+YY8ECS49v5u6r6b0keB+5PcgPwC+DDbfxe4CrgAPBb4KMAVXU0ya3A423cLVV1dB7zkiTN05zDoapeAN41pP6/gPcPqRdw4xTb2gXsmutcJEkLy29IS5I6DAdJUsdoX+KiFW3dIl2VdvD2Dy7K+0oLyT0HSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjr8Epy0wOb65bvtG47N+3byfgFPC8U9B0lSh+EgSeowHCRJHYaDJKnDcJAkdXi1kjRCvE25Fop7DpKkjiUTDkk2J/lZkgNJdiz2fCRpJVsS4ZBkFfBF4ErgYuDaJBcv7qwkaeVaKuccLgEOVNULAEnuA7YAP1nUWUmalZM517EQ3wQ/znMdp85SCYc1wIsDy4eASxdpLpKWicU6AQ+jH0xLJRwypFadQck2YFtbnEzysxm2ez7wy3nObcn7tyugT3scHaPSZz437ctLucd/NptBSyUcDgEXDiyvBQ6fOKiqdgI7Z7vRJE9U1fj8p7e0rYQ+7XF0rIQ+R6HHJXFCGngcWJ/koiRnAdcAexZ5TpK0Yi2JPYeqOpbkJuAhYBWwq6r2L/K0JGnFWhLhAFBVe4G9C7zZWR+CWuZWQp/2ODpWQp/LvsdUdc77SpJWuKVyzkGStISMbDiM4u04kuxKciTJswO185LsS/J8+3nuYs5xvpJcmOSRJM8l2Z/k460+an2+McljSX7c+vxMq1+U5NHW51fbBRrLWpJVSX6U5JtteRR7PJjkmSRPJXmi1Zb1Z3Ykw2GEb8dxD7D5hNoO4OGqWg883JaXs2PA9qp6O7AJuLH97katz9eBy6rqXcBGYHOSTcDngDtan68ANyziHBfKx4HnBpZHsUeAP6mqjQOXsC7rz+xIhgMDt+Ooqt8Bx2/HsaxV1XeBoyeUtwC72/PdwNWndVILrKpeqqoftuev0f9HZQ2j12dV1WRbPLM9CrgM+FqrL/s+k6wFPgj8p7YcRqzHaSzrz+yohsOw23GsWaS5nGpjVfUS9P9hBS5Y5PksmCTrgHcDjzKCfbbDLU8BR4B9wD8Ar1bVsTZkFD63/xH498D/actvZfR6hH6w/32SJ9udHGCZf2aXzKWsC2xWt+PQ0pXkzcDXgU9U1a/7f3COlqr6PbAxyTnAA8Dbhw07vbNaOEn+FDhSVU8mmTheHjJ02fY44H1VdTjJBcC+JD9d7AnN16juOczqdhwj4uUkqwHazyOLPJ95S3Im/WD4clV9o5VHrs/jqupVoEf/HMs5SY7/0bbcP7fvA/4syUH6h3Yvo78nMUo9AlBVh9vPI/SD/hKW+Wd2VMNhJd2OYw+wtT3fCjy4iHOZt3ZM+m7guar6wsBLo9bn29oeA0nOBj5A//zKI8CH2rBl3WdVfaqq1lbVOvr/H/xOVV3HCPUIkORNSf74+HPgcuBZlvlndmS/BJfkKvp/pRy/HcdtizyleUvyFWCC/h0fXwZuBv4rcD/wT4FfAB+uqhNPWi8bSf4V8N+BZ/h/x6k/Tf+8wyj1+S/pn6RcRf+PtPur6pYk/5z+X9nnAT8C/nVVvb54M10Y7bDSX1fVn45aj62fB9riGcDfVdVtSd7KMv7Mjmw4SJLmblQPK0mS5sFwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHf8XBxn78/gCglcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.text_length.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2idx(list_of_text):\n",
    "    words = {\"_pad_\": 0}\n",
    "    idx = 1 \n",
    "    for i in list_of_text:\n",
    "        for w in nltk.word_tokenize(i):\n",
    "            if w.lower() not in words.keys():\n",
    "                words[w.lower()] = idx\n",
    "                idx += 1\n",
    "    print(f'Number of unique tokens in the data are {len(words)}')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the data are 17290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'match'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = create_word2idx(df.sentence)\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "idx2word[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"word2indexed\"] = df.review.apply(lambda x: [words[w.lower()] for w in nltk.word_tokenize(x.strip())])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(EMBEDDING_DIM, word2idx, embedding_function):\n",
    "    matrix_len = len(list(word2idx.keys()))\n",
    "    weights_matrix = np.zeros((matrix_len, EMBEDDING_DIM), dtype=float)\n",
    "    words_found = 0\n",
    "    bar = pyprind.ProgBar(len(word2idx), bar_char='█')\n",
    "    for i, word in enumerate(list(word2idx.keys())):\n",
    "        try: \n",
    "            weights_matrix[i] = embedding_function[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        bar.update()\n",
    "    print(f\"Number of words from text found in embedding function are {words_found}\")\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from text found in embedding function are 16841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_WEIGHTS_MATRIX = create_embedding_matrix(EMBEDDING_DIM, word2idx, glove2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df=None, maxlen=10, word2idx=None):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "        self.df['sentence'] = self.df.sentence.progress_apply(lambda x: x.strip())\n",
    "        print('Indexing...')\n",
    "        self.df['sentimentidx'] = df.sentence.progress_apply(lambda x: [word2idx[w.lower()] for w in nltk.word_tokenize(x.strip())])\n",
    "        print('Calculating lengths')\n",
    "#         self.df['lengths'] = self.df.sentimentidx.progress_apply(lambda x: self.maxlen if len(x) > self.maxlen else len(x))\n",
    "        self.df[\"lengths\"] = self.df.sentimentidx.progress_apply(lambda x: len(x))\n",
    "        print('Padding')\n",
    "#         self.df['sentimentpadded'] = self.df.sentimentidx.progress_apply(self.pad_data)\n",
    "#         print(self.df.head())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        X = self.pad_data_live(idx)\n",
    "        y = self.df.label[idx]\n",
    "        return X, y\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded\n",
    "    \n",
    "    def pad_data_live(self, idx):\n",
    "#         print(idx)\n",
    "#         print(self.df.lengths[idx])\n",
    "        maxlen = max(self.df.lengths[idx])\n",
    "#         print(maxlen)\n",
    "        padded_list = []\n",
    "        for i in range(len(idx)):\n",
    "            padded = np.zeros((maxlen,), dtype=np.int64)\n",
    "            if self.df.lengths[idx[i]] > maxlen:\n",
    "                padded[:] = self.df.sentimentidx[i][:18]\n",
    "            else:\n",
    "                padded[:self.df.lengths[idx[i]]] = self.df.sentimentidx[idx[i]]\n",
    "            padded_list.append(padded)\n",
    "        return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76961, 8)\n",
      "(1821, 8)\n",
      "(872, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train = df.loc[df.split == \"train\", :].reset_index(drop=True)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = df.loc[df.split == \"test\", :].reset_index(drop=True)\n",
    "print(df_test.shape)\n",
    "\n",
    "df_dev = df.loc[df.split == \"dev\", :].reset_index(drop=True)\n",
    "print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [00:00<00:00, 374091.57it/s]\n",
      " 19%|█▊        | 341/1821 [00:00<00:00, 3399.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [00:00<00:00, 4394.44it/s]\n",
      "100%|██████████| 1821/1821 [00:00<00:00, 436173.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating lengths\n",
      "Padding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ds_train = VectorizeData(df_train, maxlen=25, word2idx=word2idx)\n",
    "ds_test = VectorizeData(df_test, maxlen=25, word2idx=word2idx)\n",
    "# ds_dev = VectorizeData(df_dev, maxlen=25, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([239, 240,  36,  23, 241, 242, 243, 244, 245, 246, 247,  16, 248,\n",
       "         249, 107, 105]),\n",
       "  array([700, 701,  11, 702,  16, 703, 704,  16, 705,  13, 706, 105,   0,\n",
       "           0,   0,   0])],\n",
       " 0    0\n",
       " 1    1\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\n",
    "from torch._six import string_classes, int_classes, FileNotFoundError\n",
    "from torch._six import container_abcs\n",
    "import random\n",
    "import torch\n",
    "import torch.multiprocessing as multiprocessing\n",
    "from torch._C import _set_worker_signal_handlers, _update_worker_pids, \\\n",
    "    _remove_worker_pids, _error_if_any_worker_fails\n",
    "from . import SequentialSampler, RandomSampler, BatchSampler\n",
    "import signal\n",
    "import functools\n",
    "from torch._six import container_abcs\n",
    "import re\n",
    "import sys\n",
    "import threading\n",
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import atexit\n",
    "from torch._six import string_classes, int_classes, FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    error_msg = \"batch must contain tensors, numbers, dicts or lists; found {}\"\n",
    "    elem_type = type(batch[0])\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        out = None\n",
    "        if _use_shared_memory:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = batch[0].storage()._new_shared(numel)\n",
    "            out = batch[0].new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            # array of string classes and object\n",
    "            if re.search('[SaUO]', elem.dtype.str) is not None:\n",
    "                raise TypeError(error_msg.format(elem.dtype))\n",
    "\n",
    "            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n",
    "        if elem.shape == ():  # scalars\n",
    "            py_type = float if elem.dtype.name.startswith('float') else int\n",
    "            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n",
    "    elif isinstance(batch[0], int_classes):\n",
    "        return torch.LongTensor(batch)\n",
    "    elif isinstance(batch[0], float):\n",
    "        return torch.DoubleTensor(batch)\n",
    "    elif isinstance(batch[0], string_classes):\n",
    "        return batchj\n",
    "    elif isinstance(batch[0], container_abcs.Mapping):\n",
    "        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n",
    "    elif isinstance(batch[0], container_abcs.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError((error_msg.format(type(batch[0]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DataLoaderIter(object):\n",
    "    def __init__(self, loader):\n",
    "        self.dataset = loader.dataset\n",
    "        self.collate_fn = loader.collate_fn\n",
    "        self.batch_sampler = loader.batch_sampler\n",
    "        self.num_workers = loader.num_workers\n",
    "        self.pin_memory = loader.pin_memory and torch.cuda.is_available()\n",
    "        self.timeout = loader.timeout\n",
    "\n",
    "        self.sample_iter = iter(self.batch_sampler)\n",
    "\n",
    "        base_seed = torch.LongTensor(1).random_().item()\n",
    "\n",
    "        if self.num_workers > 0:\n",
    "            self.worker_init_fn = loader.worker_init_fn\n",
    "            self.worker_queue_idx = 0\n",
    "            self.worker_result_queue = multiprocessing.Queue()\n",
    "            self.batches_outstanding = 0\n",
    "            self.worker_pids_set = False\n",
    "            self.shutdown = False\n",
    "            self.send_idx = 0\n",
    "            self.rcvd_idx = 0\n",
    "            self.reorder_dict = {}\n",
    "            self.done_event = multiprocessing.Event()\n",
    "\n",
    "            self.index_queues = []\n",
    "            self.workers = []\n",
    "            for i in range(self.num_workers):\n",
    "                index_queue = multiprocessing.Queue()\n",
    "                index_queue.cancel_join_thread()\n",
    "                w = multiprocessing.Process(\n",
    "                    target=_worker_loop,\n",
    "                    args=(self.dataset, index_queue,\n",
    "                          self.worker_result_queue, self.done_event,\n",
    "                          self.collate_fn, base_seed + i,\n",
    "                          self.worker_init_fn, i))\n",
    "                w.daemon = True\n",
    "                # NB: Process.start() actually take some time as it needs to\n",
    "                #     start a process and pass the arguments over via a pipe.\n",
    "                #     Therefore, we only add a worker to self.workers list after\n",
    "                #     it started, so that we do not call .join() if program dies\n",
    "                #     before it starts, and __del__ tries to join but will get:\n",
    "                #     AssertionError: can only join a started process.\n",
    "                w.start()\n",
    "                self.index_queues.append(index_queue)\n",
    "                self.workers.append(w)\n",
    "\n",
    "            if self.pin_memory:\n",
    "                self.data_queue = queue.Queue()\n",
    "                pin_memory_thread = threading.Thread(\n",
    "                    target=_pin_memory_loop,\n",
    "                    args=(self.worker_result_queue, self.data_queue,\n",
    "                          torch.cuda.current_device(), self.done_event))\n",
    "                pin_memory_thread.daemon = True\n",
    "                pin_memory_thread.start()\n",
    "                # Similar to workers (see comment above), we only register\n",
    "                # pin_memory_thread once it is started.\n",
    "                self.pin_memory_thread = pin_memory_thread\n",
    "            else:\n",
    "                self.data_queue = self.worker_result_queue\n",
    "\n",
    "            _update_worker_pids(id(self), tuple(w.pid for w in self.workers))\n",
    "            _set_SIGCHLD_handler()\n",
    "            self.worker_pids_set = True\n",
    "\n",
    "            # prime the prefetch loop\n",
    "            for _ in range(2 * self.num_workers):\n",
    "                self._put_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)\n",
    "\n",
    "    def _get_batch(self):\n",
    "        # In the non-timeout case, worker exit is covered by SIGCHLD handler.\n",
    "        # But if `pin_memory=True`, we still need account for the possibility\n",
    "        # that `pin_memory_thread` dies.\n",
    "        if self.timeout > 0:\n",
    "            try:\n",
    "                return self.data_queue.get(timeout=self.timeout)\n",
    "            except queue.Empty:\n",
    "                raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n",
    "        elif self.pin_memory:\n",
    "            while self.pin_memory_thread.is_alive():\n",
    "                try:\n",
    "                    return self.data_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "            else:\n",
    "                # while condition is false, i.e., pin_memory_thread died.\n",
    "                raise RuntimeError('Pin memory thread exited unexpectedly')\n",
    "            # In this case, `self.data_queue` is a `queue.Queue`,. But we don't\n",
    "            # need to call `.task_done()` because we don't use `.join()`.\n",
    "        else:\n",
    "            return self.data_queue.get()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.num_workers == 0:  # same-process loading\n",
    "            indices = next(self.sample_iter)  # may raise StopIteration\n",
    "#             batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "            batch = self.collate_fn(self.dataset[indices])\n",
    "            if self.pin_memory:\n",
    "                batch = pin_memory_batch(batch)\n",
    "            return batch\n",
    "\n",
    "        # check if the next sample has already been generated\n",
    "        if self.rcvd_idx in self.reorder_dict:\n",
    "            batch = self.reorder_dict.pop(self.rcvd_idx)\n",
    "            return self._process_next_batch(batch)\n",
    "\n",
    "        if self.batches_outstanding == 0:\n",
    "            self._shutdown_workers()\n",
    "            raise StopIteration\n",
    "\n",
    "        while True:\n",
    "            assert (not self.shutdown and self.batches_outstanding > 0)\n",
    "            idx, batch = self._get_batch()\n",
    "            self.batches_outstanding -= 1\n",
    "            if idx != self.rcvd_idx:\n",
    "                # store out-of-order samples\n",
    "                self.reorder_dict[idx] = batch\n",
    "                continue\n",
    "            return self._process_next_batch(batch)\n",
    "\n",
    "    next = __next__  # Python 2 compatibility\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _put_indices(self):\n",
    "        assert self.batches_outstanding < 2 * self.num_workers\n",
    "        indices = next(self.sample_iter, None)\n",
    "        if indices is None:\n",
    "            return\n",
    "        self.index_queues[self.worker_queue_idx].put((self.send_idx, indices))\n",
    "        self.worker_queue_idx = (self.worker_queue_idx + 1) % self.num_workers\n",
    "        self.batches_outstanding += 1\n",
    "        self.send_idx += 1\n",
    "\n",
    "    def _process_next_batch(self, batch):\n",
    "        self.rcvd_idx += 1\n",
    "        self._put_indices()\n",
    "        if isinstance(batch, ExceptionWrapper):\n",
    "            raise batch.exc_type(batch.exc_msg)\n",
    "        return batch\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # TODO: add limited pickling support for sharing an iterator\n",
    "        # across multiple threads for HOGWILD.\n",
    "        # Probably the best way to do this is by moving the sample pushing\n",
    "        # to a separate thread and then just sharing the data queue\n",
    "        # but signalling the end is tricky without a non-blocking API\n",
    "        raise NotImplementedError(\"_DataLoaderIter cannot be pickled\")\n",
    "\n",
    "    def _shutdown_workers(self):\n",
    "        # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on\n",
    "        # the logic of this function.\n",
    "        if _python_exit_status is True or _python_exit_status is None:\n",
    "            # See (2) of the note. If Python is shutting down, do no-op.\n",
    "            return\n",
    "        # Normal exit when last reference is gone / iterator is depleted.\n",
    "        # See (1) and the second half of the note.\n",
    "        if not self.shutdown:\n",
    "            self.shutdown = True\n",
    "            # Removes pids from the C side data structure first so worker\n",
    "            # termination afterwards won't trigger false positive error report.\n",
    "            if self.worker_pids_set:\n",
    "                _remove_worker_pids(id(self))\n",
    "                self.worker_pids_set = False\n",
    "\n",
    "            self.done_event.set()\n",
    "\n",
    "            # Exit `pin_memory_thread` first because exiting workers may leave\n",
    "            # corrupted data in `worker_result_queue` which `pin_memory_thread`\n",
    "            # reads from.\n",
    "            if hasattr(self, 'pin_memory_thread'):\n",
    "                # Use hasattr in case error happens before we set the attribute.\n",
    "                # First time do `worker_result_queue.put` in this process.\n",
    "\n",
    "                # `cancel_join_thread` in case that `pin_memory_thread` exited.\n",
    "                self.worker_result_queue.cancel_join_thread()\n",
    "                self.worker_result_queue.put(None)\n",
    "                self.pin_memory_thread.join()\n",
    "                # Indicate that no more data will be put on this queue by the\n",
    "                # current process. This **must** be called after\n",
    "                # `pin_memory_thread` is joined because that thread shares the\n",
    "                # same pipe handles with this loader thread. If the handle is\n",
    "                # closed, Py3 will error in this case, but Py2 will just time\n",
    "                # out even if there is data in the queue.\n",
    "                self.worker_result_queue.close()\n",
    "\n",
    "            # Exit workers now.\n",
    "            for q in self.index_queues:\n",
    "                q.put(None)\n",
    "                # Indicate that no more data will be put on this queue by the\n",
    "                # current process.\n",
    "                q.close()\n",
    "            for w in self.workers:\n",
    "                w.join()\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.num_workers > 0:\n",
    "            self._shutdown_workers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    __initialized = False\n",
    "\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,\n",
    "                 timeout=0, worker_init_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.collate_fn = collate_fn\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "        self.timeout = timeout\n",
    "        self.worker_init_fn = worker_init_fn\n",
    "\n",
    "        if timeout < 0:\n",
    "            raise ValueError('timeout option should be non-negative')\n",
    "\n",
    "        if batch_sampler is not None:\n",
    "            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler option is mutually exclusive '\n",
    "                                 'with batch_size, shuffle, sampler, and '\n",
    "                                 'drop_last')\n",
    "            self.batch_size = None\n",
    "            self.drop_last = None\n",
    "\n",
    "        if sampler is not None and shuffle:\n",
    "            raise ValueError('sampler option is mutually exclusive with '\n",
    "                             'shuffle')\n",
    "\n",
    "        if self.num_workers < 0:\n",
    "            raise ValueError('num_workers option cannot be negative; '\n",
    "                             'use num_workers=0 to disable multiprocessing.')\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if sampler is None:\n",
    "                if shuffle:\n",
    "                    sampler = RandomSampler(dataset)\n",
    "                else:\n",
    "                    sampler = SequentialSampler(dataset)\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.__initialized = True\n",
    "\n",
    "    def __setattr__(self, attr, val):\n",
    "        if self.__initialized and attr in ('batch_size', 'sampler', 'drop_last'):\n",
    "            raise ValueError('{} attribute should not be set after {} is '\n",
    "                             'initialized'.format(attr, self.__class__.__name__))\n",
    "\n",
    "        super(DataLoader, self).__setattr__(attr, val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return _DataLoaderIter(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# dl_train = DataLoader(dataset=ds_train, batch_size=32, shuffle=True)\n",
    "# print(len(dl_train))\n",
    "\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=32)\n",
    "print(len(dl_test))\n",
    "# 526204\n",
    "\n",
    "# dl_dev = DataLoader(dataset=ds_dev, batch_size=32, shuffle=True)\n",
    "# print(len(dl_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 6, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([700, 701,  11, 702,  16, 703, 704,  16, 705,  13, 706, 105,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       "  array([903, 904,  28, 905,  21,  23, 906,  53, 907,  28,  16, 908,  89,\n",
       "         909, 565,  21,   1, 910, 911, 105,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       "  array([ 186,   21,   23, 1155,   13, 1156, 1157,    7, 1158, 1159,    9,\n",
       "          438,  456,   28,  114,  601,  983,  321,  621, 1160,  598, 1161,\n",
       "         1162,   28,   16, 1163,   20,   21,  300,   53,  458,  445, 1164,\n",
       "         1165,  105]),\n",
       "  array([1256,  142,   20, 1257,  105,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]),\n",
       "  array([  23,  277, 1751,   13, 1196,  152, 1752, 1191,  388,  596, 1753,\n",
       "          404, 1754,  150,  116,  229, 1131, 1670,   13, 1755,  596, 1756,\n",
       "          142,  105,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]),\n",
       "  array([ 579,   28,   36,   11, 1617,   13, 1618,   54, 1619, 1620,   16,\n",
       "         1621, 1622,  450, 1200,   28,   23,  962, 1623, 1624,  429,  588,\n",
       "         1225,  222,  105,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0])],\n",
       " 1    1\n",
       " 2    0\n",
       " 3    1\n",
       " 4    0\n",
       " 6    0\n",
       " 5    1\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[[1,2,3,4,6,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-66210f0a1434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-5561b434d967>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#             batch = self.collate_fn([self.dataset[i] for i in indices])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-e6daad620b59>\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-e6daad620b59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-e6daad620b59>\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mpy_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-e6daad620b59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mpy_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got int)"
     ]
    }
   ],
   "source": [
    "for i in dl_test:\n",
    "    print(i)\n",
    "    xs,ys =  i\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(num_embeddings=len(word2idx), embedding_dim=EMBEDDING_DIM)\n",
    "# embedded = embedding(xs)\n",
    "# print(embedded.shape)\n",
    "\n",
    "# embedded = embedded.unsqueeze(1)\n",
    "# embedded.shape\n",
    "\n",
    "# co = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 1), stride=(1,1), padding=(2, 0))\n",
    "# c = co(embedded)\n",
    "# c.shape\n",
    "\n",
    "# pool_1 = torch.topk(c, 5, dim=2, largest=True)[0]\n",
    "# pool_1.shape\n",
    "\n",
    "# co_2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(2, 1), stride=(1,1), padding=(1, 0))\n",
    "# c_2 = co_2(pool_1)\n",
    "# c_2.shape\n",
    "\n",
    "# fold = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "# f = fold(c_2)\n",
    "# f.shape\n",
    "\n",
    "# pool_2 = torch.topk(f, 3, dim=2, largest=True)[0]\n",
    "# pool_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution 2d \n",
    "- kmax pooling\n",
    "- non_linear function tanh to the pooled matrix \n",
    "- second convolution\n",
    "- folding\n",
    "- kmax pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_number=1,\n",
    "        sent_length=7,\n",
    "        conv_kernel_size=(3, 1),\n",
    "        conv_input_channels=1,\n",
    "        conv_output_channels=2,\n",
    "        conv_stride=(1, 1),\n",
    "        k_max_number=5,\n",
    "        folding_kernel_size=(1, 2),\n",
    "        folding_stride=(1,1)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cell_number=cell_number \n",
    "        self.sent_length=sent_length\n",
    "        self.conv_kernel_size=conv_kernel_size\n",
    "        self.conv_input_channels=conv_input_channels\n",
    "        self.conv_output_channels=conv_output_channels\n",
    "        self.conv_stride=conv_stride\n",
    "        self.k_max_number=k_max_number\n",
    "        self.folding_kernel_size=folding_kernel_size\n",
    "        self.folding_stride=folding_stride\n",
    "        \n",
    "        # calculating padding size\n",
    "        self.pad_0_direction = math.ceil(self.conv_kernel_size[0]  - 1)\n",
    "        self.pad_1_direction = math.ceil(self.conv_kernel_size[1] - 1)\n",
    "        \n",
    "        # 2d convolution\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels=self.conv_input_channels,\n",
    "            out_channels=self.conv_output_channels,\n",
    "            kernel_size=self.conv_kernel_size,\n",
    "            stride=self.conv_stride,\n",
    "            padding=(self.pad_0_direction, self.pad_1_direction)\n",
    "        )\n",
    "        \n",
    "        # if cell is last then initialising folding\n",
    "        if cell_number == -1:\n",
    "            self.fold = nn.AvgPool2d(kernel_size=self.folding_kernel_size, stride=self.folding_stride)\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # [batch_size, input_channels, sent_length_in, embedding_dim]\n",
    "        conved = self.conv_layer(inp)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length_out, embedding_dim]\n",
    "        if self.cell_number == -1:\n",
    "            conved = self.fold(conved)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length, embedding_dim/2]\n",
    "        k_maxed = torch.tanh(torch.topk(conved, self.k_max_number, dim=2, largest=True)[0])\n",
    "        \n",
    "        # [batch_size, out_channels, k_maxed_number, embedding_dim/2]\n",
    "        return k_maxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell = DCNNCell()\n",
    "# out = cell(embedded)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_last = DCNNCell(\n",
    "#     cell_number=-1,\n",
    "#     sent_length=5,\n",
    "#     conv_kernel_size=(2, 1),\n",
    "#     conv_input_channels=2,\n",
    "#     conv_output_channels=6,\n",
    "#     conv_stride=(1, 1),\n",
    "#     k_max_number=3,\n",
    "#     folding_kernel_size=(1, 2),\n",
    "#     folding_stride=(1,2)\n",
    "# )\n",
    "# out_2 = cell_last(out)\n",
    "# out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "class DCNN_SST(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parameter_dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.parameter_dict = parameter_dict\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            embedding_dim=self.parameter_dict[\"embedding_dim\"],\n",
    "            num_embeddings=self.parameter_dict[\"vocab_length\"]\n",
    "        )\n",
    "        \n",
    "        self.dcnn_first_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_one_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_one_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_stride\"],\n",
    "        )\n",
    "        \n",
    "        self.dcnn_last_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_two_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_stride\"],\n",
    "        )\n",
    "        \n",
    "        self.fc_layer_input = self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"] *\\\n",
    "            self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"] *\\\n",
    "            math.floor(self.parameter_dict[\"embedding_dim\"]/4)\n",
    "            \n",
    "        self.dropout = nn.Dropout(self.parameter_dict[\"dropout_rate\"])\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc = nn.Linear(self.fc_layer_input, self.parameter_dict[\"output_dim\"])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # [batch_size, sent_length]\n",
    "        embedded = self.embedding(inp)\n",
    "        \n",
    "        # [batch_size, sent_length, embedding_dim]\n",
    "        # adding single channel dimension\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "#         print(embedded.shape)\n",
    "        \n",
    "        # [batch_size, 1(initial_input_channel), sent_length, embedding_dim]\n",
    "        out = self.dcnn_first_cell(embedded)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, first_cell_output_channels, first_cell_k_maxed_number, embedding_dim]\n",
    "        out = self.dcnn_last_cell(out)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        # [batch_size, last_cell_output_channels, last_cell_k_maxed_number, embedding_dim/2]\n",
    "        flat = self.dropout(self.flatten(out))\n",
    "#         print(flat.shape)\n",
    "\n",
    "        #[batch_size, last_cell_output_channels * last_cell_k_maxed_number * embedding_dim/2]\n",
    "        fc = self.fc(flat)\n",
    "#         print(fc.shape)\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell_one_parameter_dict': {'conv_input_channels': 1,\n",
      "                             'conv_kernel_size': (7, 1),\n",
      "                             'conv_output_channels': 6,\n",
      "                             'conv_stride': (1, 1),\n",
      "                             'folding_kernel_size': (1, 2),\n",
      "                             'folding_stride': (1, 2),\n",
      "                             'k_max_number': 7,\n",
      "                             'sent_length': 25},\n",
      " 'cell_two_parameter_dict': {'conv_input_channels': 6,\n",
      "                             'conv_kernel_size': (5, 1),\n",
      "                             'conv_output_channels': 14,\n",
      "                             'conv_stride': (1, 1),\n",
      "                             'folding_kernel_size': (1, 2),\n",
      "                             'folding_stride': (1, 2),\n",
      "                             'k_max_number': 4,\n",
      "                             'sent_length': 7},\n",
      " 'dropout_rate': 0.2,\n",
      " 'embedding_dim': 100,\n",
      " 'output_dim': 3,\n",
      " 'vocab_length': 17290}\n"
     ]
    }
   ],
   "source": [
    "SST_DATASET_PARAMETERS = {\n",
    "    \"cell_one_parameter_dict\" : {\n",
    "        \"sent_length\": 25,\n",
    "        \"conv_kernel_size\": (7, 1),\n",
    "        \"conv_input_channels\": 1,\n",
    "        \"conv_output_channels\": 6,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 7,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"cell_two_parameter_dict\" : {\n",
    "        \"sent_length\": None,\n",
    "        \"conv_kernel_size\": (5, 1),\n",
    "        \"conv_input_channels\": 6,\n",
    "        \"conv_output_channels\": 14,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 4,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"vocab_length\": len(word2idx),\n",
    "    \"output_dim\": 3\n",
    "}\n",
    "SST_DATASET_PARAMETERS[\"cell_two_parameter_dict\"][\"sent_length\"] = SST_DATASET_PARAMETERS[\"cell_one_parameter_dict\"][\"k_max_number\"]\n",
    "pprint(SST_DATASET_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCNN_SST(\n",
    "    parameter_dict=SST_DATASET_PARAMETERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(xs)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3527,  0.0322,  0.2399,  ...,  0.1652, -0.7237, -1.0349],\n",
       "        [-0.1663,  0.4447,  0.8156,  ..., -0.1421, -0.1757,  0.0323],\n",
       "        [ 0.0857, -0.2220,  0.1657,  ..., -0.0743,  0.7581, -0.3424],\n",
       "        ...,\n",
       "        [-0.7104,  0.4102,  0.3084,  ..., -0.6550, -0.3810, -0.2137],\n",
       "        [ 0.1277,  0.6899, -0.0773,  ...,  0.3810,  0.1500,  0.4918],\n",
       "        [-0.5264,  0.4826,  0.2927,  ..., -0.2265, -0.0978,  0.5828]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(torch.from_numpy(EMBEDDING_WEIGHTS_MATRIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model are : 1733685\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Number of trainable parameters in the model are : {}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), weight_decay=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yntY13-w6UMa"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n",
    "    correct = (ind == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-sJsioa6UMh"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#     bar = pyprind.ProgBar(100, bar_char='█')\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "#         x, y = batch\n",
    "\n",
    "        inputs, labels = batch\n",
    "        if device == \"cpu\":\n",
    "            x, y = Variable(inputs), Variable(labels.long())\n",
    "        else:\n",
    "            x, y = Variable(inputs.cuda()), Variable(labels.long().cuda())\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = calculate_accuracy(predictions, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        bar.update()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJbOdv4-6UMt"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "#         bar = pyprind.ProgBar(100, bar_char='█')\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            inputs, labels = batch\n",
    "            if device == None:\n",
    "                x, y = Variable(inputs), Variable(labels.long())\n",
    "            else:\n",
    "                x, y = Variable(inputs.cuda()), Variable(labels.long().cuda())\n",
    "            \n",
    "\n",
    "            predictions = model(x).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, y)\n",
    "            acc = calculate_accuracy(predictions, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            bar.update()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation loss did not improve\")\n",
    "    return\n",
    "\n",
    "MODEL_PATH = \"/home/neo/github_projects/dynamic_convolutional_neural_network/data/pretrained_models/{}_sst2_model.tar\".format(datetime.today().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3143
    },
    "colab_type": "code",
    "id": "qFKq12GF6UM4",
    "outputId": "a0482869-2aeb-47e7-b550-dcd8dc3a0694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:27\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 01 | Train Loss: 0.436 | Train Acc: 78.37% | Val. Loss: 0.461 | Val. Acc: 78.91% |\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 02 | Train Loss: 0.266 | Train Acc: 89.34% | Val. Loss: 0.432 | Val. Acc: 81.25% |\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 03 | Train Loss: 0.220 | Train Acc: 91.48% | Val. Loss: 0.433 | Val. Acc: 82.37% |\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 04 | Train Loss: 0.192 | Train Acc: 92.80% | Val. Loss: 0.468 | Val. Acc: 81.25% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:27\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 05 | Train Loss: 0.170 | Train Acc: 93.75% | Val. Loss: 0.464 | Val. Acc: 81.47% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 06 | Train Loss: 0.152 | Train Acc: 94.46% | Val. Loss: 0.496 | Val. Acc: 78.46% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 07 | Train Loss: 0.140 | Train Acc: 94.84% | Val. Loss: 0.494 | Val. Acc: 78.57% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:27\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 08 | Train Loss: 0.130 | Train Acc: 95.29% | Val. Loss: 0.455 | Val. Acc: 80.92% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 09 | Train Loss: 0.124 | Train Acc: 95.50% | Val. Loss: 0.509 | Val. Acc: 79.69% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:28\n",
      "0% [████████████████████████████] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Epoch: 10 | Train Loss: 0.119 | Train Acc: 95.71% | Val. Loss: 0.471 | Val. Acc: 80.13% |\n",
      "=> Validation loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "base_dev_acc = 0.0\n",
    "for epoch in range(N_EPOCHS):\n",
    "    is_best = False\n",
    "    \n",
    "    train_loss, train_acc = train(model, dl_train, optimizer, criterion, device=device)\n",
    "    valid_loss, valid_acc = evaluate(model, dl_dev, criterion, device=device)\n",
    "\n",
    "    print()\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    \n",
    "    if base_dev_acc < valid_acc:\n",
    "        is_best = True\n",
    "        base_dev_acc = valid_acc\n",
    "    \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': valid_loss,\n",
    "        'best_dev_accuracy': valid_acc\n",
    "    }, is_best, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint '/home/neo/github_projects/dynamic_convolutional_neural_network/data/pretrained_models/2019-02-03_sst2_model.tar' (trained for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "resume_weights = MODEL_PATH\n",
    "checkpoint = torch.load(resume_weights)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy = checkpoint['best_dev_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8236607142857143"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4212619155122523, 0.8165456746753893)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dl_test, criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_on_given_set(model_list, df, SENT_LENGTH, WORD2IDX, device=\"cpu\"):\n",
    "#     ds_test = VectorizeData(df_test, maxlen=SENT_LENGTH, word2idx=WORD2IDX)\n",
    "#     dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model_list = [model.eval() for model in model_list]\n",
    "    shared_model = model_list[0]\n",
    "    sentiment_specific_model = model_list[1]\n",
    "    topic_specific_model = model_list[2]\n",
    "    \n",
    "    y_test_sentiment = []\n",
    "    y_test_topic = []\n",
    "    all_prediction_sentiment = []\n",
    "    all_prediction_topic = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(dl_test), bar_char='█')\n",
    "#         bar = pyprind.ProgBar(100, bar_char='█')\n",
    "        \n",
    "        for i, batch in enumerate(dl_test):\n",
    "            inputs, labels_sentiment, labels_topic = batch\n",
    "            if device == \"cpu\":\n",
    "                x, y_sentiment, y_topic = Variable(inputs), Variable(labels_sentiment.long()), Variable(labels_topic.long())\n",
    "            else:\n",
    "                x, y_sentiment, y_topic = Variable(inputs.cuda()), Variable(labels_sentiment.long().cuda()), Variable(labels_topic.long().cuda())\n",
    "            \n",
    "            # shared model output\n",
    "            shared_model_output = shared_model(x)\n",
    "\n",
    "            sentiment_specific_output = sentiment_specific_model(shared_model_output).squeeze(1)\n",
    "            topic_specific_output = topic_specific_model(shared_model_output).squeeze(1)\n",
    "\n",
    "            predictions_sentiment = return_predictions(sentiment_specific_output)\n",
    "            predictions_topic = return_predictions(topic_specific_output)\n",
    "\n",
    "            all_prediction_sentiment.extend(predictions_sentiment.tolist())\n",
    "            all_prediction_topic.extend(predictions_topic.tolist())\n",
    "            \n",
    "            y_test_sentiment.extend(y_sentiment.tolist())\n",
    "            y_test_topic.extend(y_topic.tolist())\n",
    "            \n",
    "            bar.update()\n",
    "\n",
    "    print(\"Sentiment REsutls ::::::::::::::::::::::::;\")\n",
    "    print( 'Accuracy:', accuracy_score(y_test_sentiment, all_prediction_sentiment))\n",
    "\n",
    "    print(\"macro f1 score : {}\".format(f1_score(y_test_sentiment, all_prediction_sentiment, average='macro')))\n",
    "\n",
    "    print(\"micro f1 score : {}\".format(f1_score(y_test_sentiment, all_prediction_sentiment, average='micro')))      \n",
    "\n",
    "    print( '\\n clasification report:\\n', classification_report(y_test_sentiment, all_prediction_sentiment))\n",
    "\n",
    "    print( '\\n confusion matrix:\\n',confusion_matrix(y_test_sentiment, all_prediction_sentiment))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"TOPIC REsutls ::::::::::::::::::::::::;\")\n",
    "    print( 'Accuracy:', accuracy_score(y_test_topic, all_prediction_topic))\n",
    "\n",
    "    print(\"macro f1 score : {}\".format(f1_score(y_test_topic, all_prediction_topic, average='macro')))\n",
    "\n",
    "    print(\"micro f1 score : {}\".format(f1_score(y_test_topic, all_prediction_topic, average='micro')))      \n",
    "\n",
    "    print( '\\n clasification report:\\n', classification_report(y_test_topic, all_prediction_topic))\n",
    "\n",
    "    print( '\\n confusion matrix:\\n',confusion_matrix(y_test_topic, all_prediction_topic))\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
